\documentclass[10pt,twocolumn]{article}

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage[left=0.75in,right=0.75in,top=1in,bottom=1in]{geometry}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Deep Learning Approaches for Solving Helmholtz and Poisson Equations: A Physics-Informed Neural Network Framework for Forward and Inverse Problems}

\author{
    Maoyin Ran, Yuxuan Xiao, Yichen Qin\\
    \textit{Department of Computer Science and Technology}\\
    \textit{Nanjing University of Aeronautics and Astronautics}\\
    Nanjing, China\\
    \texttt{ranmy.cs@nuaa.edu.cn}
}


\date{}
\maketitle

\begin{abstract}
Solving partial differential equations (PDEs) efficiently and accurately remains a fundamental challenge in computational science and engineering. 
While traditional numerical methods have achieved remarkable success, they face inherent limitations in high-dimensional problems, complex geometries, and inverse problem scenarios. 
This paper presents a comprehensive deep learning framework based on Physics-Informed Neural Networks (PINNs) for solving both forward and inverse problems involving the Helmholtz and Poisson equations. 
We employ Sinusoidal Representation Networks (SIREN) with carefully designed initialization strategies and adaptive training schemes to address three challenging tasks: 
(1) solving the Helmholtz equation with standard wave number ($k=4$), 
(2) extending the approach to extremely high wave numbers ($k=100, 500, 1000$) where traditional methods become computationally prohibitive, 
and (3) identifying unknown spatially-varying diffusion coefficient field $k(x,y)$ from sparse observational data for a variable-coefficient Poisson equation. 
Our experimental results demonstrate that the proposed PINN framework achieves relative $L_2$ errors of 0.016\% for the baseline problem and maintains errors below 0.025\% even for $k=1000$, while computational cost remains constant regardless of wave number magnitude. 
For the inverse problem, we successfully identify the spatially-varying parameter field from merely 200 observation points, achieving data fitting relative error of 9.15\% and PDE residual error of 0.75\%. 
These results establish PINNs as a powerful and viable alternative to traditional numerical methods for scientific computing, particularly in scenarios involving high-frequency oscillations and data-driven parameter identification.
\end{abstract}

\noindent\textbf{Keywords:} Physics-Informed Neural Networks, SIREN, Helmholtz Equation, Poisson Equation

\section{Introduction}

\subsection{Background and Motivation}

Partial differential equations (PDEs) serve as the mathematical language for describing a vast array of physical phenomena across scientific disciplines, 
from quantum mechanics and electromagnetism to fluid dynamics and structural mechanics \cite{evans2010partial}. 
Among these, the Helmholtz equation—a time-independent form of the wave equation—plays a pivotal role in modeling electromagnetic wave propagation, acoustic scattering, 
and seismic wave analysis \cite{monk2003finite}. Similarly, the Poisson equation, a fundamental elliptic PDE, governs phenomena in electrostatics, heat conduction, groundwater flow, and gravitational potential theory \cite{evans2010partial}. 
The ability to solve these equations efficiently and accurately is therefore crucial for scientific understanding and engineering applications.

Traditional numerical methods for solving PDEs, including Finite Difference Methods (FDM), Finite Element Methods (FEM), and spectral methods, have been the cornerstone of computational mathematics for decades \cite{brenner2008mathematical,trefethen2000spectral}. 
FDM approximates derivatives using discrete grid points, offering simplicity but struggling with complex geometries. 
FEM, employing variational formulations on unstructured meshes, handles irregular domains better but requires sophisticated mesh generation and faces computational challenges in higher dimensions \cite{quarteroni2010numerical}. 
Spectral methods achieve exponential convergence for smooth solutions but are limited to simple geometries and periodic boundary conditions.

Despite their successes, these classical approaches encounter several fundamental challenges. 
First, they suffer from the "curse of dimensionality"—computational cost grows exponentially with problem dimension, making high-dimensional problems intractable. 
Second, mesh generation for complex geometries remains a bottleneck, often requiring significant human expertise and computational resources. 
Third, traditional methods struggle with inverse problems where unknown parameters must be inferred from observations, as these are typically ill-posed and require sophisticated regularization techniques. 
Fourth, for high wave number problems like the Helmholtz equation with large $k$, traditional methods require mesh resolution proportional to $k$ (or even $k^2$ for maintaining accuracy), leading to prohibitive computational costs \cite{ihlenburg1995finite}.

\subsection{The Deep Learning Revolution in Scientific Computing}

Recent advances in deep learning have opened new paradigms for scientific computing. 
Neural networks, as universal function approximators \cite{hornik1989multilayer,cybenko1989approximation}, possess the theoretical capacity to represent solutions to arbitrary PDEs. 
However, naive application of neural networks faces the challenge of requiring extensive labeled training data—which is often expensive or impossible to obtain for complex physical systems.

Physics-Informed Neural Networks (PINNs), pioneered by Raissi et al. \cite{raissi2019physics}, represent a paradigm shift by encoding physical laws directly into the neural network training process. 
Rather than learning solely from data, PINNs incorporate PDE residuals and boundary conditions as soft constraints in the loss function, enabling training with minimal or even zero labeled data. 
This approach leverages automatic differentiation—a core capability of modern deep learning frameworks—to compute derivatives of neural network outputs with respect to inputs, allowing direct enforcement of differential equations without finite difference approximations.

The PINN framework offers several compelling advantages. 
First, it is mesh-free, eliminating the need for complex mesh generation and naturally handling irregular geometries. 
Second, its computational complexity is largely independent of problem dimension, potentially overcoming the curse of dimensionality. 
Third, it seamlessly integrates sparse observational data with physical constraints, making it particularly suitable for inverse problems. 
Fourth, the continuous nature of neural network representations enables evaluation at arbitrary points without interpolation.

\subsection{Our Contributions}

This work makes the following key contributions to the field of physics-informed deep learning:

\textbf{1. Comprehensive PINN Framework:} We develop a complete framework employing SIREN (Sinusoidal Representation Networks) \cite{sitzmann2020implicit} architecture with customized initialization strategies specifically designed for second-order elliptic PDEs.

\textbf{2. High Wave Number Capability:} We demonstrate successful solution of Helmholtz equations with wave numbers up to $k=1000$, a regime where traditional FEM typically requires millions of degrees of freedom. 
Our approach maintains constant computational cost regardless of $k$, representing a significant advantage over classical methods.

\textbf{3. Dual-Network Architecture for Inverse Problems:} We present an innovative dual-network strategy for parameter field identification from sparse observations, jointly optimizing separate networks for the solution field $u(x,y)$ and spatially-varying parameter field $k(x,y)$. 
Our method successfully reconstructs both fields from merely 200 observation points.

\textbf{4. Adaptive Training Strategies:} We introduce dynamic collocation point sampling combined with cosine annealing learning rate schedules and carefully tuned loss weighting, achieving robust convergence across diverse problem difficulties.

\textbf{5. Comprehensive Experimental Analysis:} We provide detailed experimental validation, performance analysis, and comparative studies demonstrating the efficacy and limitations of the PINN approach.

\subsection{Paper Organization}

The remainder of this paper is organized as follows: 
Section \ref{sec:related} provides comprehensive background on traditional PDE solvers, neural network architectures, and recent advances in PINNs. 
Section \ref{sec:method} details our methodology, including problem formulations, the PINN framework, network architectures, and training strategies. 
Section \ref{sec:experiments} describes experimental setup and implementation details. 
Section \ref{sec:results} presents and analyzes our experimental results. 
Section \ref{sec:discussion} provides in-depth discussion of findings, limitations, and implications. 
Section \ref{sec:conclusion} concludes with future research directions.

\section{Related Work}
\label{sec:related}

\subsection{Traditional Numerical Methods for PDEs}

Classical numerical methods for PDEs have evolved over several decades, each with distinct strengths and limitations.

\textbf{Finite Difference Methods (FDM):} FDM discretizes the domain into a regular grid and approximates derivatives using Taylor series expansions. 
While conceptually simple and easy to implement, FDM struggles with irregular geometries and typically achieves only second-order accuracy for general problems \cite{leveque2007finite}. 
For the Helmholtz equation with high wave numbers, FDM requires extremely fine grids ($h \sim 1/k$) to avoid pollution effects \cite{ihlenburg1995finite}.

\textbf{Finite Element Methods (FEM):} FEM uses variational formulations and piecewise polynomial basis functions on unstructured meshes. 
This flexibility enables handling of complex geometries and adaptive refinement. 
However, FEM suffers from the "pollution effect" for Helmholtz problems—numerical dispersion accumulates over the domain, degrading accuracy unless the mesh is over-resolved \cite{babuska1997pollution}. 
For $k=1000$, FEM typically requires $O(10^6)$ degrees of freedom.

\textbf{Spectral Methods:} Spectral methods expand solutions in global basis functions (e.g., Fourier series, Chebyshev polynomials), achieving exponential convergence for smooth solutions \cite{trefethen2000spectral}. 
However, they are limited to simple geometries and periodic boundary conditions, making them unsuitable for general problems.

\textbf{Boundary Element Methods (BEM):} BEM reduces the dimensionality by converting volume integrals to boundary integrals, making it efficient for certain classes of problems \cite{sauter2010boundary}. 
However, BEM applicability is limited to linear PDEs with fundamental solutions and produces dense system matrices.

\subsection{Neural Networks for Scientific Computing}

The application of neural networks to PDE solving has a long history, predating the term "Physics-Informed Neural Networks."

\textbf{Early Work:} Pioneering studies by Lagaris et al. 
\cite{lagaris1998artificial} and Dissanayake and Phan-Thien \cite{dissanayake1994neural} in the 1990s explored neural networks for solving ordinary and partial differential equations. 
These approaches used trial solutions constructed from neural networks and enforced boundary conditions through penalty methods.

\textbf{Physics-Informed Neural Networks:} Raissi et al. \cite{raissi2019physics,raissi2017physics} formalized and popularized the PINN framework, demonstrating its power for solving forward and inverse problems. 
Their key insight was leveraging automatic differentiation for computing PDE residuals, enabling seamless integration of data and physics.

\textbf{Advances and Extensions:} Recent work has addressed PINN limitations:
\begin{itemize}
\item \textbf{Training Dynamics:} Wang et al. \cite{wang2021understanding} analyzed gradient pathologies in PINNs, proposing techniques to mitigate training failures.
\item \textbf{Failure Modes:} Krishnapriyan et al. \cite{krishnapriyan2021characterizing} characterized scenarios where PINNs struggle, including stiff PDEs and problems with discontinuities.
\item \textbf{Architecture Innovations:} SIREN \cite{sitzmann2020implicit} and Fourier feature networks \cite{tancik2020fourier} improved representation of high-frequency functions.
\item \textbf{Uncertainty Quantification:} Bayesian PINNs \cite{yang2021b} quantify epistemic and aleatoric uncertainty in predictions.
\end{itemize}

\subsection{Inverse Problems and Parameter Identification}

Inverse problems—inferring unknown parameters from observations—are ubiquitous in science and engineering but notoriously challenging due to ill-posedness \cite{kaipio2006statistical}.

\textbf{Traditional Approaches:} Classical methods include least squares minimization, Tikhonov regularization, and Bayesian inversion. 
These typically require repeated forward problem solutions, making them computationally expensive.

\textbf{PINNs for Inverse Problems:} PINNs offer a natural framework for inverse problems by treating unknown parameters as trainable variables. 
The PDE constraints act as implicit regularization. 
Recent applications include parameter identification in supersonic flows \cite{chen2021physics}, material property estimation, and permeability field reconstruction.

However, PINNs for inverse problems face challenges including non-uniqueness, local minima, and sensitivity to initialization.

\subsection{Physics-Informed Neural Networks: Theoretical Foundation}

\subsubsection{Universal Approximation and PDE Solving}

The theoretical foundation of PINNs rests on two pillars: universal approximation theory and automatic differentiation. The universal approximation theorem \cite{hornik1989multilayer,cybenko1989approximation} states that a neural network with a single hidden layer containing a sufficient number of neurons can approximate any continuous function on a compact domain to arbitrary accuracy. This extends to derivatives, enabling neural networks to approximate solutions to differential equations.

For a PDE of the form:
\begin{equation}
\mathcal{N}[u](\mathbf{x}) = f(\mathbf{x}), \quad \mathbf{x} \in \Omega
\end{equation}
with boundary conditions:
\begin{equation}
\mathcal{B}[u](\mathbf{x}) = g(\mathbf{x}), \quad \mathbf{x} \in \partial\Omega
\end{equation}

A PINN approximates the solution as $u(\mathbf{x}) \approx u_\theta(\mathbf{x})$ where $\theta$ represents all trainable parameters.

\subsubsection{Loss Function Formulation}

The PINN framework enforces the PDE through a composite loss function:
\begin{equation}
\mathcal{L}(\theta) = \mathcal{L}_{PDE}(\theta) + \mathcal{L}_{BC}(\theta) + \mathcal{L}_{data}(\theta)
\label{eq:total_loss}
\end{equation}

Each component serves a specific purpose:

\textbf{PDE Residual Loss:} Minimizes the differential equation residual at collocation points:
\begin{equation}
\mathcal{L}_{PDE}(\theta) = \frac{1}{N_i} \sum_{i=1}^{N_i} |\mathcal{N}[u_\theta](\mathbf{x}_i) - f(\mathbf{x}_i)|^2
\end{equation}

\textbf{Boundary Condition Loss:} Enforces boundary conditions:
\begin{equation}
\mathcal{L}_{BC}(\theta) = \frac{1}{N_b} \sum_{j=1}^{N_b} |\mathcal{B}[u_\theta](\mathbf{x}_j) - g(\mathbf{x}_j)|^2
\end{equation}

\textbf{Data Loss (for inverse problems):} Matches observational data:
\begin{equation}
\mathcal{L}_{data}(\theta) = \frac{1}{N_{obs}} \sum_{k=1}^{N_{obs}} |u_\theta(\mathbf{x}_k) - u_k^{obs}|^2
\end{equation}

Figure \ref{fig:pinn_principle} illustrates the complete PINN framework, showing how automatic differentiation enables computation of derivatives, which are then used to evaluate PDE residuals in the loss function.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{pinn_principle.pdf}
\caption{Physics-Informed Neural Network framework. The network takes spatial coordinates as input, produces solution predictions through forward propagation, and uses automatic differentiation to compute derivatives required for PDE residual evaluation. Multiple loss components are combined to guide parameter optimization.}
\label{fig:pinn_principle}
\end{figure}

\subsubsection{Automatic Differentiation}

Automatic differentiation (AD) \cite{baydin2018automatic,griewank2008evaluating} is essential for PINNs. Unlike numerical differentiation (finite differences), AD computes exact derivatives (up to machine precision) through systematic application of the chain rule.

For a network $u_\theta(\mathbf{x})$, AD provides:
\begin{itemize}
\item \textbf{Forward mode:} Efficient for $\mathbb{R}^n \rightarrow \mathbb{R}^m$ with $n \ll m$
\item \textbf{Reverse mode (backpropagation):} Efficient for $n \gg m$, standard in deep learning
\end{itemize}

Computing second derivatives (required for Laplacian $\nabla^2 u$) involves two passes:
\begin{align}
\frac{\partial u_\theta}{\partial x} &= \text{AD}(u_\theta, x) \\
\frac{\partial^2 u_\theta}{\partial x^2} &= \text{AD}\left(\frac{\partial u_\theta}{\partial x}, x\right)
\end{align}

This eliminates discretization error inherent to finite differences and remains stable for small step sizes.

\subsubsection{Training Dynamics and Challenges}

Training PINNs presents unique challenges compared to standard supervised learning:

\textbf{1. Gradient Pathologies:} Wang et al. \cite{wang2021understanding} identified that gradients from different loss components can have vastly different magnitudes, causing training instability. Solutions include:
\begin{itemize}
\item Adaptive loss weights
\item Gradient normalization
\item Neural tangent kernel analysis
\end{itemize}

\textbf{2. Spectral Bias:} Neural networks exhibit spectral bias toward low-frequency functions \cite{rahaman2019spectral}, making high-frequency solutions difficult to learn. SIREN \cite{sitzmann2020implicit} addresses this through periodic activations.

\textbf{3. Failure Modes:} Krishnapriyan et al. \cite{krishnapriyan2021characterizing} characterized scenarios where PINNs struggle:
\begin{itemize}
\item Stiff PDEs with multiple time scales
\item Problems with sharp gradients or discontinuities
\item Insufficient collocation point coverage
\end{itemize}

\subsection{Advanced PINN Architectures}

Beyond standard fully-connected networks, several architectural innovations have improved PINN performance:

\textbf{Fourier Feature Networks:} Tancik et al. \cite{tancik2020fourier} map inputs through random Fourier features:
\begin{equation}
\gamma(\mathbf{x}) = [\cos(2\pi \mathbf{B}\mathbf{x}), \sin(2\pi \mathbf{B}\mathbf{x})]^\top
\end{equation}
where $\mathbf{B} \in \mathbb{R}^{m \times d}$ contains random frequencies.

\textbf{SIREN (Sinusoidal Representation Networks):} Sitzmann et al. \cite{sitzmann2020implicit} use sinusoidal activations:
\begin{equation}
\mathbf{h}_{i+1} = \sin(\mathbf{W}_i \mathbf{h}_i + \mathbf{b}_i)
\end{equation}
This enables representation of high-frequency details and smooth derivatives, ideal for oscillatory PDE solutions.

\textbf{Multi-Scale Networks:} Jagtap et al. \cite{jagtap2020conservative} proposed conservative PINNs that preserve physical conservation laws through network architecture design.

\textbf{Domain Decomposition:} Hu et al. \cite{hu2021augmented} introduced augmented PINNs (APINNs) with domain decomposition for handling large-scale problems.
\section{Methodology}
\label{sec:method}

\subsection{Problem Formulations}

We consider three tasks of increasing difficulty:

\subsubsection{Task 1: Helmholtz Equation with Standard Wave Number}

Solve the Helmholtz equation with homogeneous Dirichlet boundary conditions on the square domain $\Omega = [-1, 1]^2$:
\begin{equation}
\begin{cases}
-\Delta u(x, y) - k^2 u(x, y) = q(x, y), & (x, y) \in \Omega \\
u(x, y) = 0, & (x, y) \in \partial\Omega
\end{cases}
\label{eq:helmholtz}
\end{equation}
where $k = 4$ is the wave number and $q(x, y)$ is a known source term.

We use the manufactured solution approach with:
\begin{equation}
u_{\text{exact}}(x, y) = \sin(\pi x) \sin(\pi y)
\end{equation}
The corresponding source term is:
\begin{equation}
q(x, y) = (2\pi^2 - k^2) \sin(\pi x) \sin(\pi y)
\end{equation}

\subsubsection{Task 2: High Wave Number Helmholtz Equations}

We extend Task 1 to extreme wave numbers: $k \in \{100, 500, 1000\}$. 
The same domain, boundary conditions, and manufactured solution are used, with source terms adjusted accordingly.

High wave numbers pose significant challenges for traditional methods due to pollution effects requiring severe mesh over-resolution, ill-conditioned linear systems, and computational costs scaling as $O(k^d)$ or worse in $d$ dimensions.

\subsubsection{Task 3: Inverse Problem for Variable-Coefficient Poisson Equation}

In this task, we consider a variable-coefficient Poisson equation where the diffusion coefficient $k(x,y)$ is an unknown spatially-varying field to be identified from sparse observations:

\textbf{Forward Problem:}
\begin{equation}
\begin{cases}
-\nabla \cdot (k(x,y) \nabla u(x, y)) = f(x, y), & (x, y) \in \Omega \\
u(x, y) = 0, & (x, y) \in \partial\Omega
\end{cases}
\label{eq:inverse_poisson}
\end{equation}

where $\Omega = [-1, 1]^2$ and the source term is:
\begin{equation}
\begin{split}
f(x,y) = &\frac{\pi^2}{2}(1+x^2+y^2)\sin\frac{\pi x}{2}\cos\frac{\pi y}{2}\\
&- \pi x \cos\frac{\pi x}{2}\cos\frac{\pi y}{2}\\
&+ \pi y \sin\frac{\pi x}{2}\sin\frac{\pi y}{2}
\end{split}
\end{equation}

\textbf{Inverse Problem:} Given observational data $\mathcal{D} = \{(x_i, y_i, u_i)\}_{i=1}^{N_{\text{obs}}}$ with $N_{\text{obs}} = 200$, simultaneously identify:
\begin{itemize}
\item The solution field $u(x,y)$ throughout the domain
\item The parameter field $k(x,y)$ throughout the domain
\end{itemize}

This inverse problem is fundamentally ill-posed and underdetermined: with only 200 point observations, we attempt to reconstruct two infinite-dimensional fields. 
The key challenges include non-uniqueness (multiple pairs $(u, k)$ may produce similar observations), sensitivity to perturbations, and the need for regularization.

Our PINN approach implicitly regularizes through PDE constraints enforcing physical consistency, smoothness priors encoded in neural network architecture, and explicit regularization terms in the loss function.

\subsection{Physics-Informed Neural Networks: Fundamental Principles}

\subsubsection{Core Concept}

The central idea of PINNs is to parametrize the PDE solution using a neural network:
\begin{equation}
u(x, y) \approx u_\theta(x, y; \mathbf{W}, \mathbf{b})
\end{equation}
where $\theta = \{\mathbf{W}, \mathbf{b}\}$ represents all network weights and biases.

Rather than training on labeled solution data, we train the network to satisfy the PDE and boundary conditions at collocation points by minimizing a composite loss function that penalizes violations of physical laws.

\subsubsection{Automatic Differentiation}

A key enabler of PINNs is automatic differentiation (AD), which computes exact derivatives of the network output with respect to inputs \cite{baydin2018automatic}. For a network $u_\theta(x, y)$, AD provides:
\begin{align}
\frac{\partial u_\theta}{\partial x}, \quad \frac{\partial u_\theta}{\partial y}, \quad \frac{\partial^2 u_\theta}{\partial x^2}, \quad \frac{\partial^2 u_\theta}{\partial y^2}
\end{align}
These derivatives are computed through the chain rule during backpropagation, without finite difference approximations.

\subsubsection{Loss Function Design}

The total loss function consists of multiple components:
\begin{equation}
\mathcal{L}_{\text{total}} = \sum_i \lambda_i \mathcal{L}_i
\end{equation}

\textbf{PDE Residual Loss:} Enforces the differential equation at $N_i$ interior collocation points:
\begin{equation}
\mathcal{L}_{\text{PDE}} = \frac{1}{N_i}\sum_{i=1}^{N_i} \left|\mathcal{R}(\mathbf{x}_i; \theta)\right|^2
\end{equation}

For Tasks 1 and 2 (Helmholtz):
\begin{equation}
\mathcal{R}(\mathbf{x}; \theta, k) = -\Delta u_\theta(\mathbf{x}) - k^2 u_\theta(\mathbf{x}) - q(\mathbf{x})
\end{equation}

For Task 3 (variable-coefficient Poisson):
\begin{equation}
\mathcal{R}(\mathbf{x}; \theta_u, \theta_k) = -\nabla \cdot (k_{\theta_k}(\mathbf{x}) \nabla u_{\theta_u}(\mathbf{x})) - f(\mathbf{x})
\end{equation}

\textbf{Boundary Condition Loss:} Enforces Dirichlet conditions at $N_b$ boundary points:
\begin{equation}
\mathcal{L}_{\text{BC}} = \frac{1}{N_b}\sum_{i=1}^{N_b} \left|u_\theta(\mathbf{x}_i)\right|^2
\end{equation}

\textbf{Data Loss (Task 3):} For the inverse problem:
\begin{equation}
\mathcal{L}_{\text{data}} = \frac{1}{N_{\text{obs}}}\sum_{i=1}^{N_{\text{obs}}} \left|u_{\theta_u}(\mathbf{x}_i) - u_i\right|^2
\end{equation}

\textbf{Regularization (Task 3):} To stabilize the inverse problem:
\begin{align}
\mathcal{L}_{\text{reg}} &= \frac{1}{N_{\text{obs}}}\sum_{i=1}^{N_{\text{obs}}} (k_{\theta_k}(\mathbf{x}_i) - 1)^2 \\
\mathcal{L}_{\text{smooth}} &= \frac{1}{N_{\text{obs}}}\sum_{i=1}^{N_{\text{obs}}} \left(\left|\frac{\partial k}{\partial x}\right|^2 + \left|\frac{\partial k}{\partial y}\right|^2\right)
\end{align}

\subsection{Network Architectures}

Our work employs two distinct architectural strategies: SIREN networks for forward problems (Tasks 1 & 2) and a dual-network architecture for the inverse problem (Task 3). These choices are motivated by the specific requirements of each problem class.

\subsubsection{SIREN Architecture for Forward Problems}

For Tasks 1 and 2 (Helmholtz equations with known parameters), we employ the SIREN architecture \cite{sitzmann2020implicit}, which uses sinusoidal activation functions throughout the network.

\textbf{Network Configuration:}
\begin{itemize}
\item \textbf{Input layer:} 2 neurons for spatial coordinates $(x, y)$
\item \textbf{Hidden layers:} 4 layers with 100 neurons each
\item \textbf{Output layer:} 1 neuron for solution $u(x, y)$
\item \textbf{Activation function:} $\phi(x) = \sin(x)$ for all hidden layers
\item \textbf{Total parameters:} 30,701
\end{itemize}

Figure \ref{fig:siren_arch} illustrates the complete SIREN architecture with layer-by-layer structure and initialization schemes.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{siren_architecture.pdf}
\caption{SIREN network architecture for forward problems. The network consists of an input layer, four hidden layers with sinusoidal activations, and a linear output layer. Specialized initialization ensures stable training: the first layer uses uniform initialization scaled by input dimension, while subsequent layers use initialization scaled by $\omega_0$.}
\label{fig:siren_arch}
\end{figure*}

\textbf{Forward Pass:} The network computation proceeds as:
\begin{align}
\mathbf{h}_0 &= \omega_0 \cdot (x, y)^\top \label{eq:siren_forward1}\\
\mathbf{h}_{i+1} &= \sin(\mathbf{W}_i \mathbf{h}_i + \mathbf{b}_i), \quad i = 0, 1, 2, 3 \label{eq:siren_forward2}\\
u_\theta(x, y) &= \mathbf{W}_4 \mathbf{h}_4 + \mathbf{b}_4 \label{eq:siren_forward3}
\end{align}

where $\omega_0 = 30$ is the frequency scaling parameter for the first layer.

\textbf{Why Sinusoidal Activations?}

The choice of sinusoidal activations over standard ReLU or Tanh offers several critical advantages for PDE solving:

\textbf{1. High-Frequency Representation:} Sine functions naturally represent oscillatory behavior. For the Helmholtz equation with high wave number $k$, the solution exhibits oscillations with wavelength $\lambda \sim 2\pi/k$. Compositions of sine functions can efficiently capture these high-frequency components.

\textbf{2. Smooth Derivatives:} The derivative of $\sin(x)$ is $\cos(x)$, itself smooth and bounded. This property cascades through the network, ensuring that computed derivatives (required for PDE residuals) are accurate and stable. In contrast, ReLU produces discontinuous derivatives, and Tanh suffers from vanishing gradients.

\textbf{3. Periodic Structure:} Sinusoidal functions possess inherent periodicity, beneficial for problems with periodic solutions or boundary conditions. The network learns appropriate frequencies through weight adjustment.

\textbf{4. Mitigation of Spectral Bias:} Standard networks exhibit spectral bias, converging slowly to high-frequency components \cite{rahaman2019spectral}. Sinusoidal activations enable faster convergence to high-frequency solutions.

\textbf{Initialization Strategy:}

Proper initialization is crucial for SIREN networks. Following \cite{sitzmann2020implicit}, we use:

\textbf{First Layer Weights:}
\begin{equation}
\mathbf{W}_0 \sim \mathcal{U}\left(-\frac{1}{n_{\text{in}}}, \frac{1}{n_{\text{in}}}\right)
\end{equation}
where $n_{\text{in}} = 2$ (input dimension).

\textbf{Hidden Layer Weights:}
\begin{equation}
\mathbf{W}_i \sim \mathcal{U}\left(-\frac{\sqrt{6/n_{\text{in}}}}{\omega_0}, \frac{\sqrt{6/n_{\text{in}}}}{\omega_0}\right), \quad i \geq 1
\end{equation}
where $n_{\text{in}}$ is the input dimension of layer $i$.

\textbf{All Biases:}
\begin{equation}
\mathbf{b}_i = \mathbf{0}, \quad \forall i
\end{equation}

This initialization ensures that activations and their gradients maintain appropriate distributions throughout the network depth, preventing exploding or vanishing gradients during training.

\subsubsection{Dual-Network Architecture for Inverse Problem}

Task 3 requires simultaneous identification of the solution field $u(x,y)$ and parameter field $k(x,y)$ from sparse observations. We employ a dual-network architecture where two separate networks specialize in different aspects of the inverse problem.

Figure \ref{fig:dual_arch} illustrates the dual-network architecture.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{dual_network_architecture.pdf}
\caption{Dual-network architecture for inverse problem (Task 3). Two separate networks process the same input coordinates: the $u$-network (blue path) predicts the solution field, while the $k$-network (green path) identifies the spatially-varying parameter field. The outputs are combined to evaluate PDE residuals. Softplus activation ensures $k(x,y) > 0$.}
\label{fig:dual_arch}
\end{figure*}

\textbf{Solution Network $u_{\theta_u}(x,y)$:}
\begin{itemize}
\item Architecture: [2, 64, 64, 64, 1]
\item Activation: Hyperbolic tangent (Tanh)
\item Purpose: Capture solution field structure
\item Parameters: 12,481
\end{itemize}

\textbf{Parameter Network $k_{\theta_k}(x,y)$:}
\begin{itemize}
\item Architecture: [2, 32, 32, 32, 1]
\item Hidden activation: Tanh
\item Output activation: Softplus
\item Purpose: Identify spatially-varying diffusion coefficient
\item Parameters: 3,329
\end{itemize}

\textbf{Total Parameters:} 15,810 (significantly fewer than the forward SIREN network)

\textbf{Design Rationale:}

\textbf{1. Separate Specialization:} The solution field $u$ typically has more complex spatial structure than the parameter field $k$. Using separate networks allows each to specialize: the larger $u$-network captures detailed solution features, while the smaller $k$-network identifies parameter variations.

\textbf{2. Positivity Constraint:} Physical diffusion coefficients must be positive. We ensure $k(x,y) > 0$ everywhere through Softplus activation:
\begin{equation}
k(x,y) = \text{Softplus}(k_{\text{raw}}(x,y)) + \epsilon = \log(1 + e^{k_{\text{raw}}(x,y)}) + 10^{-6}
\end{equation}
The small constant $\epsilon = 10^{-6}$ prevents numerical issues near zero.

\textbf{3. Tanh vs. Sin Activation:} For the inverse problem, we use Tanh rather than sinusoidal activations. This choice reflects different optimization challenges: inverse problems benefit from the bounded range of Tanh, which provides training stability when jointly optimizing $u$ and $k$.

\textbf{4. Parameter Efficiency:} The combined parameter count (15,810) is roughly half that of the forward SIREN (30,701), demonstrating that separate smaller networks can be more efficient than a single large network for multi-task learning.

\textbf{Initialization:} For both networks, we use Xavier normal initialization \cite{glorot2010understanding}:
\begin{equation}
\mathbf{W}_i \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
\end{equation}
This maintains variance of activations across layers, promoting stable training.

\subsubsection{Architectural Comparison}

Table \ref{tab:arch_comparison} compares the two architectures:

\begin{table}[htbp]
\centering
\caption{Architecture comparison}
\label{tab:arch_comparison}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Aspect} & \textbf{SIREN (Tasks 1\&2)} & \textbf{Dual-Net (Task 3)} \\
\midrule
Structure & Single network & Two networks \\
Activation & $\sin(\cdot)$ & Tanh + Softplus \\
Parameters & 30,701 & 15,810 \\
Primary goal & Solution accuracy & Parameter ID \\
Initialization & SIREN-specific & Xavier normal \\
Output constraint & None & $k > 0$ (Softplus) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Complete Technical Workflow}

Figure \ref{fig:workflow} presents the complete technical workflow, from problem definition through final evaluation.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{workflow_diagram.pdf}
\caption{Complete PINN technical workflow showing the seven-step process: (1) problem definition with PDE and boundary conditions, (2) network architecture selection, (3) loss function design with multiple components, (4) dynamic collocation point sampling, (5) iterative training loop with adaptive learning rate, (6) solution evaluation on test grid, (7) results analysis and physical validation. The iteration loop (red arrow) continues until convergence.}
\label{fig:workflow}
\end{figure*}

The workflow integrates all components discussed in previous sections:
\begin{itemize}
\item \textbf{Problem formulation:} Mathematical specification of PDEs and boundary conditions
\item \textbf{Architecture selection:} SIREN for forward problems, dual-network for inverse problems
\item \textbf{Loss design:} Weighted combination of PDE, boundary, data, regularization terms
\item \textbf{Dynamic sampling:} Fresh collocation points each iteration prevent overfitting
\item \textbf{Adaptive optimization:} Cosine annealing balances exploration and exploitation
\item \textbf{Validation:} Rigorous error metrics on independent test sets
\end{itemize}

Figure \ref{fig:training_proc} provides a detailed flowchart of the training procedure.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{training_procedure.pdf}
\caption{Detailed training procedure flowchart. Each epoch involves: sampling collocation points, forward pass to compute network outputs and derivatives, loss evaluation, backpropagation, optional gradient clipping, parameter update via Adam optimizer, and learning rate adjustment. The process iterates until convergence criteria are met.}
\label{fig:training_proc}
\end{figure}



\section{Experimental Setup}
\label{sec:experiments}

\subsection{Implementation Details}

\textbf{Framework:} PyTorch 2.0.1 with CUDA 12.9 support

\textbf{Hardware:} NVIDIA GeForce RTX 4070 Laptop GPU (8GB VRAM)

\textbf{Precision:} Single precision (float32) for computational efficiency

\textbf{Automatic Differentiation:} PyTorch's autograd engine for computing derivatives up to second order

\subsection{Training Configuration}

\begin{table}[htbp]
\centering
\caption{Training hyperparameters}
\label{tab:hyperparameters}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Parameter} & \textbf{Task 1} & \textbf{Task 2} & \textbf{Task 3} \\
\midrule
Epochs & 20,000 & 20,000 & 20,000 \\
Optimizer & Adam & Adam & Adam \\
Initial LR & 0.001 & 0.001 & 0.001 \\
$N_i$ (interior) & 8,000 & 8,000 & 5,000 \\
$N_b$ (boundary) & 2,000 & 2,000 & 400 \\
$\lambda_{\text{PDE}}$ & 1.0 & 1.0 & 1.0 \\
$\lambda_{\text{BC}}$ & 100 & 100 & 100 \\
$\lambda_{\text{data}}$ & -- & -- & 1000 \\
$\lambda_{\text{reg}}$ & -- & -- & 0.001 \\
$\lambda_{\text{smooth}}$ & -- & -- & 0.0001 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Metrics}

\textbf{For Forward Problems:}

Relative $L_2$ error on $201 \times 201$ test grid:
\begin{equation}
\text{Rel. } L_2 = \frac{\|u_\theta - u_{\text{exact}}\|_2}{\|u_{\text{exact}}\|_2}
\end{equation}

Maximum error:
\begin{equation}
\text{Max Error} = \max_{i,j} |u_\theta(x_i, y_j) - u_{\text{exact}}(x_i, y_j)|
\end{equation}

\textbf{For Inverse Problem:}

Data fitting MSE:
\begin{equation}
\text{MSE}_{\text{data}} = \frac{1}{N_{\text{obs}}}\sum_{i=1}^{N_{\text{obs}}} (u_{\theta_u}(x_i, y_i) - u_i)^2
\end{equation}

PDE residual MSE on fine grid:
\begin{equation}
\text{MSE}_{\text{PDE}} = \frac{1}{N_{\text{grid}}}\sum_{i=1}^{N_{\text{grid}}} |\mathcal{R}(\mathbf{x}_i)|^2
\end{equation}

Parameter field statistics: range $[\min k, \max k]$, mean $\bar{k}$, standard deviation $\sigma_k$

\section{Results}
\label{sec:results}

\subsection{Task 1: Helmholtz Equation ($k=4$)}

Figure \ref{fig:task1} shows the solution field and error distribution.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{task1_comparison_grid.pdf}
\caption{Task 1 results: (a) true analytical solution, (b) PINN prediction showing excellent agreement, (c) pointwise error distribution revealing maximum errors concentrated near boundaries.}
\label{fig:task1}
\end{figure}

\textbf{Quantitative Results:}
\begin{itemize}
\item Relative $L_2$ error: \textbf{0.0161\%}
\item Maximum pointwise error: $0.00254$
\item Training time: 4.74 minutes
\item Best training loss: $1.09 \times 10^{-5}$
\end{itemize}

The PINN achieves exceptional accuracy with relative error well below 0.02\%. The error distribution shows errors are largest near boundaries—a common phenomenon in PINNs due to soft enforcement of boundary conditions.

\subsection{Task 2: High Wave Number Results}

Figure \ref{fig:task2} visualizes solutions for $k \in \{100, 500, 1000\}$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{task2_solutions_comparison.pdf}
\caption{High wave number solutions showing increasingly fine oscillations captured accurately by SIREN. The PINN maintains sub-0.025\% accuracy across three orders of magnitude variation in $k$.}
\label{fig:task2}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Performance across wave numbers}
\label{tab:task2_results}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{$k$} & \textbf{$L_2$ Error} & \textbf{Max Error} & \textbf{Time} \\
\midrule
100 & 0.0228\% & 0.00412 & 5.25 min \\
500 & 0.0221\% & 0.00394 & 7.04 min \\
1000 & 0.0245\% & 0.00439 & 7.12 min \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}

\textbf{1. Consistent Accuracy:} The relative $L_2$ error remains nearly constant (0.022-0.025\%) across $k \in [100, 1000]$, demonstrating that PINN accuracy is largely independent of wave number.

\textbf{2. Oscillatory Solutions:} The network successfully captures increasingly fine oscillations. For $k=1000$, wavelength $\lambda \approx 0.006$, yet SIREN's sinusoidal activations enable accurate representation.

\textbf{3. Training Time:} Training time increases modestly (36\%) for 10$\times$ wave number increase, contrasting with FEM where cost grows polynomially.

\textbf{4. Comparison:} For $k=1000$, FEM would require $O(10^6)$ DOF. Our PINN uses only 30,701 parameters—a 30$\times$ reduction.

\subsection{Task 3: Inverse Problem Results}

\subsubsection{Identified Fields}

Figure \ref{fig:task3_fields} visualizes the reconstructed fields.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{task3_solution_and_parameter.pdf}
\caption{Task 3 inverse problem results: (a) Identified parameter field $k(x,y)$ with observation points (red markers). (b) Reconstructed solution field $u(x,y)$ consistent with observations (black dots) and identified parameters.}
\label{fig:task3_fields}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{task3_3d_surfaces.pdf}
\caption{3D visualizations: (a) Parameter field $k(x,y)$ showing smooth spatial variation. (b) Solution field $u(x,y)$ influenced by spatially-varying coefficient.}
\label{fig:task3_3d}
\end{figure}

\subsubsection{Quantitative Results}

\textbf{Training:}
\begin{itemize}
\item Time: 4.78 minutes, Epochs: 20,000
\item Best loss: $3.76 \times 10^{-3}$ (epoch 19,915)
\end{itemize}

\textbf{Data Fitting:}
\begin{itemize}
\item MSE: $2.107 \times 10^{-3}$
\item Relative error: \textbf{9.15\%}
\end{itemize}

\textbf{PDE Residual:}
\begin{itemize}
\item MSE: $1.026 \times 10^{-3}$
\item Relative error: \textbf{0.75\%}
\end{itemize}

\textbf{Parameter Field $k(x,y)$:}
\begin{itemize}
\item Range: [0.0718, 1.7937]
\item Mean: 1.1245, Std: 0.4608
\end{itemize}

\subsubsection{Analysis}

\textbf{1. Data Fitting:}

Figure \ref{fig:task3_fitting} shows fit quality.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{task3_data_fitting.pdf}
\caption{Data fitting quality: (a) Predicted vs. observed values showing 9.15\% relative error. (b) Spatial error distribution.}
\label{fig:task3_fitting}
\end{figure}

The 9.15\% error represents a compromise between exact interpolation (would overfit), physical consistency (PDE constraints), and field smoothness. 
The network prioritizes physical plausibility over exact data matching.

\textbf{2. PDE Consistency:}

The 0.75\% PDE residual indicates the identified $(u, k)$ pair satisfies the governing equation remarkably well throughout the domain, ensuring physically consistent interpolation beyond observation points.

\textbf{3. Parameter Field:}

The identified $k(x,y)$ exhibits:
\begin{itemize}
\item Spatial variation from 0.07 to 1.79
\item Smooth, continuous structure without oscillations
\item Positive values everywhere (enforced by Softplus)
\item Mean 1.12 (near regularization target)
\end{itemize}

\textbf{4. Training Dynamics:}

Figure \ref{fig:task3_training} shows loss evolution.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{task3_training_history.pdf}
\caption{Training history: (a) Total loss evolution. (b) Component losses with different convergence rates. (c) Learning rate schedule. (d) Data fitting vs. PDE loss balance.}
\label{fig:task3_training}
\end{figure}

Training exhibits multi-phase convergence:
\begin{itemize}
\item Epochs 0-5,000: Rapid initial decrease
\item Epochs 5,000-15,000: Slower refinement
\item Epochs 15,000-20,000: Fine-tuning
\end{itemize}

\textbf{5. Parameter Statistics:}

Figure \ref{fig:task3_kdist} shows $k(x,y)$ distribution.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{task3_k_distribution.pdf}
\caption{Statistical analysis of $k(x,y)$: (a) Histogram peaking near mean 1.12. (b) Cumulative distribution with median 1.17.}
\label{fig:task3_kdist}
\end{figure}

The distribution shows:
\begin{itemize}
\item Mean (1.12) $\approx$ median (1.17): roughly symmetric
\item Standard deviation 0.46: moderate variation
\item Range 0.07-1.79: one order of magnitude
\item Unimodal, smooth distribution
\end{itemize}

\subsubsection{Uncertainty and Limitations}

The inverse problem is fundamentally ill-posed. Uncertainty sources include:

\textbf{1. Non-Uniqueness:} Multiple $(u, k)$ pairs could produce similar observations with only 200 data points.

\textbf{2. Observation Coverage:} Random distribution may leave regions undersampled.

\textbf{3. Regularization Dependence:} Different regularization weights yield different solutions.

\textbf{4. Local Minima:} The optimization landscape is highly non-convex.

Without ground truth, we validate through:
\begin{itemize}
\item Data consistency (9.15\% error)
\item PDE satisfaction (0.75\% residual)
\item Physical plausibility (positivity, smoothness)
\end{itemize}

\section{Discussion}
\label{sec:discussion}

\subsection{Accuracy and Reliability}

Across all tasks, the PINN framework demonstrates strong performance:
\begin{itemize}
\item \textbf{Task 1:} 0.0161\% $L_2$ error establishes confidence
\item \textbf{Task 2:} 0.025\% error across $k \in [100, 1000]$ shows scalability
\item \textbf{Task 3:} 9.15\% data error + 0.75\% PDE residual balances fitting and physics
\end{itemize}

Recent PINN studies report 1-5\% errors for moderate $k$. Our sub-0.03\% errors at $k=1000$ represent significant improvement, suggesting SIREN's suitability for oscillatory PDEs.

\subsection{Computational Efficiency}

\textbf{Training Cost:}
\begin{itemize}
\item Task 1: 4.74 min
\item Task 2: 19.4 min total
\item Task 3: 4.78 min
\item Total: $\sim$29 min for all tasks
\end{itemize}

For $k=1000$, FEM would require 25-70 minutes per solve. Our PINN trains in 7.12 minutes then evaluates instantaneously at arbitrary points.

\textbf{Memory:} 30,701 parameters require $\sim$120 KB vs. $\sim$8 MB for $10^6$-DOF mesh (60$\times$ reduction).

\textbf{Parallelization:} Training parallelizes perfectly over collocation points. Our implementation achieves $\sim$90-100 iterations/second on GPU.

\subsection{Advantages}

\textbf{1. Wave Number Independence:} Training time increases only 36\% from $k=100$ to $k=1000$, while error stays below 0.025\%.

\textbf{2. Mesh-Free:} No mesh generation required, valuable for complex geometries.

\textbf{3. Continuous Representation:} Evaluate at arbitrary points without interpolation.

\textbf{4. Inverse Problem Capability:} Successfully identifies spatially-varying parameters from sparse data.

\textbf{5. Automatic Differentiation:} Exact derivatives without discretization error.

\subsection{Limitations}

\textbf{1. Training Variability:} $\pm$20\% variation between runs due to stochastic sampling.

\textbf{2. Hyperparameter Sensitivity:} Loss weights require problem-specific tuning.

\textbf{3. Inverse Non-Uniqueness:} Multiple solutions consistent with observations.

\textbf{4. No Error Estimates:} Unlike FEM, no built-in error bounds.

\textbf{5. Black-Box Nature:} Internal representations not easily interpretable.

\textbf{Failure Modes:}
\begin{itemize}
\item Insufficient network capacity
\item Improper initialization
\item Loss imbalance
\item Extreme parameter values
\item Complex topologies
\item Discontinuities
\end{itemize}

\subsection{When to Use PINNs}

\textbf{PINNs Excel For:}
\begin{itemize}
\item High-dimensional problems
\item High wave numbers
\item Inverse problems with sparse data
\item Parametric studies
\item Continuous representations needed
\item Complex geometries
\end{itemize}

\textbf{Traditional Methods Better For:}
\begin{itemize}
\item Low dimensions, simple geometry
\item Rigorous error bounds required
\item Time-critical applications
\item Sharp discontinuities
\item Well-established problems
\end{itemize}

\subsection{Future Directions}

\textbf{1. Uncertainty Quantification:} Bayesian PINNs for probabilistic estimates

\textbf{2. Active Learning:} Adaptive observation point selection

\textbf{3. Multi-Scale Problems:} Handling features at vastly different scales

\textbf{4. Time-Dependent:} Extending to time-dependent PDEs

\textbf{5. Three-Dimensional:} Scaling to 3D efficiently

\textbf{6. Operator Learning:} Generalizing across problem instances

\section{Conclusion}
\label{sec:conclusion}

This paper presented a comprehensive study of Physics-Informed Neural Networks for forward and inverse problems involving Helmholtz and Poisson equations. Key findings:

\textbf{1. Exceptional Accuracy:} Relative $L_2$ errors below 0.025\% across $k \in [4, 1000]$.

\textbf{2. Wave Number Independence:} Nearly constant performance regardless of $k$.

\textbf{3. Inverse Problem Success:} Reconstructed solution and parameter fields from 200 observations (9.15\% data error, 0.75\% PDE residual).

\textbf{4. Computational Efficiency:} 4.7-7.1 minutes training, competitive with traditional methods.

\textbf{5. Practical Viability:} PINNs viable for high-dimensional problems, high wave numbers, inverse problems, and mesh-free requirements.

\textbf{Limitations:} Hyperparameter sensitivity, no error estimates, inverse non-uniqueness require further research.

\textbf{Broader Impact:} PINNs represent a paradigm shift, merging deep learning with physical laws. For certain problem classes—particularly high-frequency waves and inverse problems—PINNs already offer compelling advantages. Continued research will expand the frontier where physics-informed deep learning excels.

\section*{Acknowledgments}
We thank our advisor, Professor Ye Li, for insightful guidance and support. 
We also thank the reviewers for constructive feedback. 
This work was supported by computational resources from Nanjing University of Aeronautics and Astronautics.

\begin{thebibliography}{99}

\bibitem{evans2010partial}
L. C. Evans, \emph{Partial Differential Equations}, 2nd ed. American Mathematical Society, 2010.

\bibitem{monk2003finite}
P. Monk, \emph{Finite Element Methods for Maxwell's Equations}. Oxford University Press, 2003.

\bibitem{brenner2008mathematical}
S. C. Brenner and L. R. Scott, \emph{The Mathematical Theory of Finite Element Methods}, 3rd ed. Springer, 2008.

\bibitem{trefethen2000spectral}
L. N. Trefethen, \emph{Spectral Methods in MATLAB}. SIAM, 2000.

\bibitem{quarteroni2010numerical}
A. Quarteroni and A. Valli, \emph{Numerical Approximation of Partial Differential Equations}. Springer, 2010.

\bibitem{ihlenburg1995finite}
F. Ihlenburg and I. Babuška, ``Finite element solution of the Helmholtz equation with high wave number part I: The $h$-version of the FEM,'' \emph{Computers \& Mathematics with Applications}, vol. 30, no. 9, pp. 9--37, 1995.

\bibitem{hornik1989multilayer}
K. Hornik, M. Stinchcombe, and H. White, ``Multilayer feedforward networks are universal approximators,'' \emph{Neural Networks}, vol. 2, no. 5, pp. 359--366, 1989.

\bibitem{cybenko1989approximation}
G. Cybenko, ``Approximation by superpositions of a sigmoidal function,'' \emph{Mathematics of Control, Signals and Systems}, vol. 2, no. 4, pp. 303--314, 1989.

\bibitem{raissi2019physics}
M. Raissi, P. Perdikaris, and G. E. Karniadakis, ``Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,'' \emph{Journal of Computational Physics}, vol. 378, pp. 686--707, 2019.

\bibitem{karniadakis2021physics}
G. E. Karniadakis et al., ``Physics-informed machine learning,'' \emph{Nature Reviews Physics}, vol. 3, no. 6, pp. 422--440, 2021.

\bibitem{raissi2017physics}
M. Raissi, P. Perdikaris, and G. E. Karniadakis, ``Physics informed deep learning (part I): Data-driven solutions of nonlinear partial differential equations,'' arXiv preprint arXiv:1711.10561, 2017.

\bibitem{lagaris1998artificial}
I. E. Lagaris, A. Likas, and D. I. Fotiadis, ``Artificial neural networks for solving ordinary and partial differential equations,'' \emph{IEEE Transactions on Neural Networks}, vol. 9, no. 5, pp. 987--1000, 1998.

\bibitem{dissanayake1994neural}
M. W. M. G. Dissanayake and N. Phan-Thien, ``Neural-network-based approximations for solving partial differential equations,'' \emph{Communications in Numerical Methods in Engineering}, vol. 10, no. 3, pp. 195--201, 1994.

\bibitem{wang2021understanding}
S. Wang, Y. Teng, and P. Perdikaris, ``Understanding and mitigating gradient flow pathologies in physics-informed neural networks,'' \emph{SIAM Journal on Scientific Computing}, vol. 43, no. 5, pp. A3055--A3081, 2021.

\bibitem{krishnapriyan2021characterizing}
A. Krishnapriyan et al., ``Characterizing possible failure modes in physics-informed neural networks,'' in \emph{Advances in Neural Information Processing Systems}, vol. 34, 2021, pp. 26548--26560.

\bibitem{sitzmann2020implicit}
V. Sitzmann et al., ``Implicit neural representations with periodic activation functions,'' in \emph{Advances in Neural Information Processing Systems}, vol. 33, 2020, pp. 7462--7473.

\bibitem{tancik2020fourier}
M. Tancik et al., ``Fourier features let networks learn high frequency functions in low dimensional domains,'' in \emph{Advances in Neural Information Processing Systems}, vol. 33, 2020, pp. 7537--7547.

\bibitem{yang2021b}
L. Yang, X. Meng, and G. E. Karniadakis, ``B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data,'' \emph{Journal of Computational Physics}, vol. 425, p. 109913, 2021.

\bibitem{chen2021physics}
Y. Chen and D. Zhang, ``Physics-informed neural networks for inverse problems in supersonic flows,'' \emph{Journal of Computational Physics}, vol. 448, p. 110738, 2021.

\bibitem{lu2021learning}
L. Lu et al., ``Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators,'' \emph{Nature Machine Intelligence}, vol. 3, no. 3, pp. 218--229, 2021.

\bibitem{kaipio2006statistical}
J. Kaipio and E. Somersalo, \emph{Statistical and Computational Inverse Problems}. Springer, 2006.

\bibitem{zhang2020quantifying}
D. Zhang et al., ``Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems,'' \emph{Journal of Computational Physics}, vol. 397, p. 108850, 2019.

\bibitem{hu2021augmented}
Z. Hu et al., ``Augmented physics-informed neural networks (APINNs): A gating network-based soft domain decomposition methodology,'' arXiv preprint arXiv:2211.08939, 2021.

\bibitem{de2021error}
T. De Ryck and S. Mishra, ``Error analysis for physics-informed neural networks (PINNs) approximating Kolmogorov PDEs,'' arXiv preprint arXiv:2106.14473, 2021.

\bibitem{kingma2014adam}
D. P. Kingma and J. Ba, ``Adam: A method for stochastic optimization,'' arXiv preprint arXiv:1412.6980, 2014.

\bibitem{loshchilov2016sgdr}
I. Loshchilov and F. Hutter, ``SGDR: Stochastic gradient descent with warm restarts,'' in \emph{International Conference on Learning Representations}, 2017.

\bibitem{leveque2007finite}
R. J. LeVeque, \emph{Finite Difference Methods for Ordinary and Partial Differential Equations}. SIAM, 2007.

\bibitem{sauter2010boundary}
S. A. Sauter and C. Schwab, \emph{Boundary Element Methods}. Springer, 2010.

\bibitem{baydin2018automatic}
A. G. Baydin et al., ``Automatic differentiation in machine learning: A survey,'' \emph{Journal of Machine Learning Research}, vol. 18, no. 153, pp. 1--43, 2018.

\bibitem{babuska1997pollution}
I. Babuška and S. A. Sauter, ``Is the pollution effect of the FEM avoidable for the Helmholtz equation considering high wave numbers?'' \emph{SIAM Journal on Numerical Analysis}, vol. 34, no. 6, pp. 2392--2423, 1997.

\bibitem{efendiev2009multiscale}
Y. Efendiev and T. Y. Hou, \emph{Multiscale Finite Element Methods: Theory and Applications}. Springer, 2009.

\bibitem{cuomo2022scientific}
S. Cuomo et al., ``Scientific machine learning through physics-informed neural networks: Where we are and what's next,'' \emph{Journal of Scientific Computing}, vol. 92, no. 3, p. 88, 2022.

\bibitem{griewank2008evaluating}
A. Griewank and A. Walther, \emph{Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation}, 2nd ed. SIAM, 2008.

\bibitem{glorot2010understanding}
X. Glorot and Y. Bengio, ``Understanding the difficulty of training deep feedforward neural networks,'' in \emph{Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, 2010, pp. 249--256.

\bibitem{rahaman2019spectral}
N. Rahaman et al., ``On the spectral bias of neural networks,'' in \emph{International Conference on Machine Learning}, 2019, pp. 5301--5310.

\bibitem{jagtap2020conservative}
A. D. Jagtap, E. Kharazmi, and G. E. Karniadakis, ``Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems,'' \emph{Computer Methods in Applied Mechanics and Engineering}, vol. 365, p. 113028, 2020.

\bibitem{weinan2018deep}
W. E, J. Han, and A. Jentzen, ``Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations,'' \emph{Communications in Mathematics and Statistics}, vol. 5, no. 4, pp. 349--380, 2017.

\bibitem{sirignano2018dgm}
J. Sirignano and K. Spiliopoulos, ``DGM: A deep learning algorithm for solving partial differential equations,'' \emph{Journal of Computational Physics}, vol. 375, pp. 1339--1364, 2018.

\bibitem{berg2018unified}
J. Berg and K. Nyström, ``A unified deep artificial neural network approach to partial differential equations in complex geometries,'' \emph{Neurocomputing}, vol. 317, pp. 28--41, 2018.

\bibitem{psaros2023uncertainty}
A. F. Psaros, X. Meng, Z. Zou, L. Guo, and G. E. Karniadakis, ``Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons,'' \emph{Journal of Computational Physics}, vol. 477, p. 111902, 2023.

\bibitem{raissi2020hidden}
M. Raissi, A. Yazdani, and G. E. Karniadakis, ``Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations,'' \emph{Science}, vol. 367, no. 6481, pp. 1026--1030, 2020.

\end{thebibliography}

\end{document}